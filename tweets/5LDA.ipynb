{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       date  \\\n",
      "0       2020-01-12 12:30:00   \n",
      "1       2020-01-22 03:00:16   \n",
      "2       2020-01-24 10:57:46   \n",
      "3       2020-01-27 06:00:07   \n",
      "4       2020-01-27 08:00:12   \n",
      "...                     ...   \n",
      "258162  2020-12-28 13:15:06   \n",
      "258163  2020-12-28 14:00:06   \n",
      "258164  2020-12-28 14:45:02   \n",
      "258165  2020-12-28 16:15:06   \n",
      "258166  2020-12-28 16:32:03   \n",
      "\n",
      "                                                     text    loc author  \n",
      "0       The diving bell spider or water spider is the ...  CHINA   CCTV  \n",
      "1       Faced with the fastchanging situation of the n...  CHINA   CCTV  \n",
      "2       The central China metropolitan of Wuhan will f...  CHINA   CCTV  \n",
      "3       Medical teams from across China left for Wuhan...  CHINA   CCTV  \n",
      "4       Many places across China have taken strict con...  CHINA   CCTV  \n",
      "...                                                   ...    ...    ...  \n",
      "258162  Stock futures rose after President Trump signe...     US    WSJ  \n",
      "258163  A citizen journalist in China who documented h...     US    WSJ  \n",
      "258164  Stimulus checks jobless aid and PPP smallbusin...     US    WSJ  \n",
      "258165  Novavax is starting a new clinical trial of up...     US    WSJ  \n",
      "258166  With masks social distancing and millions of y...     US    WSJ  \n",
      "\n",
      "[258167 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "tweet=pd.read_csv(\"D:/安装包/clean_tweets.txt\",sep='\\t',names=['date','text','loc','author'])\n",
    "print(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#coding: utf-8\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from wordcloud import STOPWORDS  #停用词\n",
    "\n",
    "#补充停用词\n",
    "STOPWORDS.add('covid')  \n",
    "STOPWORDS.add('coronavirus')\n",
    "STOPWORDS.add('covid19')\n",
    "STOPWORDS.add('corona')\n",
    "STOPWORDS.add('rt')\n",
    "STOPWORDS.add('pandemic')\n",
    "STOPWORDS.add('trump')\n",
    "\n",
    "STOPWORDS.add('france')  \n",
    "STOPWORDS.add('china')\n",
    "STOPWORDS.add('uk')\n",
    "STOPWORDS.add('us')\n",
    "STOPWORDS.add('germany')\n",
    "STOPWORDS.add('brazil')\n",
    "STOPWORDS.add('spain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------  第一步 读取数据(已分词)  ----------------------\n",
    "corpus = []\n",
    "\n",
    "# 读取预料 一行预料为一个文档\n",
    "for index,line in tweet.iterrows():\n",
    "    cor=str(line['text'])\n",
    "    corpus.append(cor.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aren', 'couldn', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'let', 'll', 'mustn', 're', 'shan', 'shouldn', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "#-----------------------  第二步 计算TF-IDF值  ----------------------- \n",
    "# 设置特征数\n",
    "n_features = 2000\n",
    "\n",
    "tf_vectorizer = TfidfVectorizer(strip_accents = 'unicode',\n",
    "                                max_features=n_features,\n",
    "                                stop_words=set(STOPWORDS),\n",
    "                                max_df = 0.99,\n",
    "                                min_df = 0.002) #去除文档内出现几率过大或过小的词汇\n",
    "\n",
    "tf = tf_vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#-------------------------  第三步 LDA分析  ------------------------ \n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# 设置主题数\n",
    "n_topics = 4\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=n_topics,\n",
    "                                max_iter=100,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50,\n",
    "                                random_state=0)\n",
    "lda.fit(tf)\n",
    "\n",
    "# 显示主题数 model.topic_word_\n",
    "print(lda.components_)\n",
    "# 几个主题就是几行 多少个关键词就是几列 \n",
    "print(lda.components_.shape)                         \n",
    "\n",
    "# 计算困惑度\n",
    "print(u'困惑度：')\n",
    "print(lda.perplexity(tf,sub_sampling = False))\n",
    "\n",
    "# 主题-关键词分布\n",
    "def print_top_words(model, tf_feature_names, n_top_words):\n",
    "    for topic_idx,topic in enumerate(model.components_):  # lda.component相当于model.topic_word_\n",
    "        print('Topic #%d:' % topic_idx)\n",
    "        print(' '.join([tf_feature_names[i] for i in topic.argsort()[:-n_top_words-1:-1]]))\n",
    "        print(\"\")\n",
    "\n",
    "# 定义好函数之后 暂定每个主题输出前10个关键词\n",
    "n_top_words = 10                                       \n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "# 调用函数\n",
    "print_top_words(lda, tf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.core.frame import DataFrame\n",
    "res = lda.transform(tf)\n",
    "res=DataFrame(res)\n",
    "tweet['class']=res.idxmax(1)\n",
    "tweet.head(20)\n",
    "\n",
    "#tweet.to_csv('/content/drive/MyDrive/yuchuli/LDA.txt',sep='\\t',header=None,index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "#保存模型\n",
    "model_file = \"D:/安装包/1tweet/lda_model4.pk\"\n",
    "joblib.dump(lda,model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:315: UserWarning: Trying to unpickle estimator LatentDirichletAllocation from version 0.22.2.post1 when using version 0.24.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "#加载训练好的模型\n",
    "import joblib\n",
    "model_file = \"D:/安装包/LDA/lda_model3.pk\"\n",
    "#词向量\n",
    "lda = joblib.load(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
      "topic                                                \n",
      "3     -0.076437 -0.362042       1        1  29.122820\n",
      "2     -0.083798  0.157872       2        1  25.149057\n",
      "0     -0.205844  0.155942       3        1  25.034437\n",
      "1      0.366079  0.048229       4        1  20.693687, topic_info=         Term         Freq        Total Category  logprob  loglift\n",
      "144     cases  5254.000000  5254.000000  Default  30.0000  30.0000\n",
      "229    deaths  3721.000000  3721.000000  Default  29.0000  29.0000\n",
      "532     masks  2710.000000  2710.000000  Default  28.0000  28.0000\n",
      "500  lockdown  6072.000000  6072.000000  Default  27.0000  27.0000\n",
      "629  patients  2018.000000  2018.000000  Default  26.0000  26.0000\n",
      "..        ...          ...          ...      ...      ...      ...\n",
      "577      need   917.724812  1108.926903   Topic4  -4.8827   1.3861\n",
      "336     first  1444.993993  2965.010303   Topic4  -4.4288   0.8566\n",
      "840     study   940.965468  1312.781579   Topic4  -4.8577   1.2423\n",
      "500  lockdown  1359.723495  6072.887251   Topic4  -4.4896   0.0788\n",
      "496      live  1015.488521  2555.968815   Topic4  -4.7815   0.6523\n",
      "\n",
      "[203 rows x 6 columns], token_table=      Topic      Freq       Term\n",
      "term                            \n",
      "15        3  0.999574       2020\n",
      "20        1  0.999083         24\n",
      "29        1  0.876558  according\n",
      "29        3  0.123158  according\n",
      "32        2  0.998887     across\n",
      "...     ...       ...        ...\n",
      "968       2  0.308875      world\n",
      "968       3  0.186227      world\n",
      "975       4  0.999531      wuhan\n",
      "977       2  0.179958       year\n",
      "977       3  0.819634       year\n",
      "\n",
      "[224 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[4, 3, 1, 2])\n",
      "Serving to http://127.0.0.1:8889/    [Ctrl-C to exit]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [20/May/2021 11:12:10] \"GET / HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "#------------------------  第四步 可视化分析  ------------------------- \n",
    "import pandas\n",
    "import numpy\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "\n",
    "#pyLDAvis.enable_notebook()\n",
    "\n",
    "data = pyLDAvis.sklearn.prepare(lda,tf,tf_vectorizer)\n",
    "print(data)\n",
    "\n",
    "#显示图形\n",
    "pyLDAvis.show(data,local=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
